{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "from torchbeast import utils\n",
    "from torchbeast.core import models\n",
    "from torchbeast import polybeast_learner as polybeast\n",
    "\n",
    "# path to flags.savedir/flags.xpid/model.tar\n",
    "checkpointpath = \"/root/logs/torchbeast/latest/model.tar\"\n",
    "\n",
    "checkpoint_states = torch.load(checkpointpath)\n",
    "\n",
    "flags = checkpoint_states[\"flags\"]\n",
    "\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "    \n",
    "flags = dotdict(flags)\n",
    "\n",
    "checkpoint_states = torch.load(checkpointpath, map_location=flags.learner_device)\n",
    "\n",
    "dataset_uses_color = flags.dataset not in [\"mnist\", \"omniglot\"]\n",
    "grayscale = dataset_uses_color and not flags.use_color\n",
    "\n",
    "env_uses_color = flags.use_color or flags.env_type == \"fluid\"\n",
    "if env_uses_color is False:\n",
    "    grayscale = True\n",
    "else:\n",
    "    grayscale = is_color and not dataset_uses_color\n",
    "\n",
    "if flags.condition:\n",
    "    dataset = utils.create_dataset(flags.dataset, grayscale)\n",
    "else:\n",
    "    dataset = None\n",
    "\n",
    "env_name, config = utils.parse_flags(flags)\n",
    "env = utils.create_env(env_name, config, grayscale=True, dataset=dataset)\n",
    "\n",
    "grid_width = 32\n",
    "\n",
    "obs_shape = env.observation_space[\"canvas\"].shape\n",
    "action_shape = env.action_space.nvec\n",
    "order = env.order\n",
    "\n",
    "model = models.Net(\n",
    "    obs_shape=obs_shape,\n",
    "    order=order,\n",
    "    action_shape=action_shape,\n",
    "    grid_shape=(grid_width, grid_width),\n",
    ").eval()\n",
    "model = model.to(flags.learner_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "checkpoint_states = torch.load(checkpointpath, map_location=flags.learner_device)\n",
    "model.load_state_dict(checkpoint_states[\"model_state_dict\"])\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "for k in obs.keys():\n",
    "    obs[k] = torch.from_numpy(obs[k]).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "done = torch.tensor(False).view(1, 1)\n",
    "\n",
    "core_state = model.initial_state()\n",
    "\n",
    "for i in range(flags.episode_length - 1):\n",
    "    (action, logits, baseline), core_state = model(obs, done, core_state)\n",
    "    action = torch.flatten(action.cpu(), 0, 1).int().numpy()\n",
    "    \n",
    "    obs, reward, done, info = env.step(action)\n",
    "    \n",
    "    for k in obs.keys():\n",
    "        tensor = torch.from_numpy(obs[k])\n",
    "        # prev_action's original shape has batch dimension.\n",
    "        if k == \"prev_action\":\n",
    "            obs[k] = tensor.unsqueeze(0)\n",
    "        else:\n",
    "            obs[k] = tensor.unsqueeze(0).unsqueeze(0)\n",
    "            \n",
    "    done = torch.tensor(done).view(1, 1)\n",
    "    \n",
    "img = obs[\"canvas\"].view(obs_shape).permute(1, 2, 0).numpy()\n",
    "\n",
    "if flags.condition:\n",
    "    target = img[..., 1:2]\n",
    "    img = img[..., 0:1]\n",
    "    \n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(img, cmap='gray', vmin=0.0, vmax=1.0, interpolation=\"nearest\")\n",
    "\n",
    "if flags.condition:\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(target, cmap='gray', vmin=0.0, vmax=1.0, interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def sample():\n",
    "    obs = env.reset()\n",
    "\n",
    "    for k in obs.keys():\n",
    "        obs[k] = torch.from_numpy(obs[k]).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    done = torch.tensor(False).view(1, 1)\n",
    "\n",
    "    core_state = model.initial_state()\n",
    "\n",
    "    for i in range(flags.episode_length - 1):\n",
    "        (action, logits, baseline), core_state = model(obs, done, core_state)\n",
    "        action = torch.flatten(action.cpu(), 0, 1).int().numpy()\n",
    "    \n",
    "        obs, reward, done, info = env.step(action)\n",
    "    \n",
    "        for k in obs.keys():\n",
    "            tensor = torch.from_numpy(obs[k])\n",
    "            # prev_action's original shape has batch dimension.\n",
    "            if k == \"prev_action\":\n",
    "                obs[k] = tensor.unsqueeze(0)\n",
    "            else:\n",
    "                obs[k] = tensor.unsqueeze(0).unsqueeze(0)\n",
    "            \n",
    "        done = torch.tensor(done).view(1, 1)\n",
    "        \n",
    "    return obs[\"canvas\"].view(obs_shape)\n",
    "\n",
    "# load model from checkpoint path.\n",
    "checkpoint_states = torch.load(checkpointpath, map_location=flags.learner_device)\n",
    "model.load_state_dict(checkpoint_states[\"model_state_dict\"])\n",
    "\n",
    "renders = [sample() for _ in range(flags.batch_size)]\n",
    "renders = torch.stack(renders)\n",
    "\n",
    "if flags.condition:\n",
    "    targets = renders[:, 1:2, ...]\n",
    "    renders = renders[:, 0:1, ...]\n",
    "    \n",
    "img = make_grid(\n",
    "    renders, nrow=math.ceil(flags.batch_size ** 0.5)\n",
    ").permute(1, 2, 0).numpy()\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.imshow(img, cmap='gray', interpolation=\"nearest\")\n",
    "\n",
    "if flags.condition:\n",
    "    target_img = make_grid(\n",
    "        targets, nrow=math.ceil(flags.batch_size ** 0.5)\n",
    "    ).permute(1, 2, 0).numpy()\n",
    "\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.imshow(target_img, cmap='gray', interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def sample_history():\n",
    "    obs = env.reset()\n",
    "\n",
    "    for k in obs.keys():\n",
    "        obs[k] = torch.from_numpy(obs[k]).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    done = torch.tensor(False).view(1, 1)\n",
    "\n",
    "    core_state = model.initial_state()\n",
    "    \n",
    "    render = obs[\"canvas\"].view(obs_shape)\n",
    "    \n",
    "    if flags.condition:\n",
    "        target = render[1:2]\n",
    "        render = render[0:1]\n",
    "\n",
    "    renders = [render]\n",
    "    \n",
    "    for i in range(flags.episode_length - 1):\n",
    "        (action, logits, baseline), core_state = model(obs, done, core_state)\n",
    "        action = torch.flatten(action.cpu(), 0, 1).int().numpy()\n",
    "    \n",
    "        obs, reward, done, info = env.step(action)\n",
    "    \n",
    "        for k in obs.keys():\n",
    "            tensor = torch.from_numpy(obs[k])\n",
    "            # prev_action's original shape has batch dimension.\n",
    "            if k == \"prev_action\":\n",
    "                obs[k] = tensor.unsqueeze(0)\n",
    "            else:\n",
    "                obs[k] = tensor.unsqueeze(0).unsqueeze(0)\n",
    "            \n",
    "        done = torch.tensor(done).view(1, 1)\n",
    "        \n",
    "        render = obs[\"canvas\"].view(obs_shape)\n",
    "        if flags.condition:\n",
    "            render = render[0:1]\n",
    "            \n",
    "        renders.append(render)\n",
    "    \n",
    "    if flags.condition:\n",
    "        renders.append(target)\n",
    "        \n",
    "    return torch.stack(renders)\n",
    "\n",
    "# load model from checkpoint path.\n",
    "checkpoint_states = torch.load(checkpointpath, map_location=flags.learner_device)\n",
    "model.load_state_dict(checkpoint_states[\"model_state_dict\"])\n",
    "\n",
    "renders = [sample_history() for _ in range(flags.batch_size)]\n",
    "\n",
    "if flags.condition:\n",
    "    nrow = math.ceil(\n",
    "        (flags.batch_size / (flags.episode_length + 1)) ** 0.5\n",
    "    ) * (flags.episode_length + 1)\n",
    "else:\n",
    "    nrow = math.ceil(\n",
    "        (flags.batch_size / flags.episode_length) ** 0.5\n",
    "    ) * flags.episode_length\n",
    "\n",
    "img = make_grid(torch.cat(renders), nrow=nrow).permute(1, 2, 0).numpy()\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(img, cmap='gray', interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "replay_buffer = polybeast.ReplayBuffer(flags.replay_buffer_size)\n",
    "replay_buffer.load_checkpoint(checkpoint_states[\"replay_buffer\"])\n",
    "\n",
    "sample = replay_buffer.sample(flags.batch_size)\n",
    "\n",
    "if flags.condition:\n",
    "    target = sample[:,1:2]\n",
    "    sample = sample[:, 0:1]\n",
    "    \n",
    "img = make_grid(\n",
    "    sample, \n",
    "    nrow=math.ceil(flags.batch_size ** 0.5)\n",
    ").permute(1, 2, 0).numpy()\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.imshow(img, cmap=\"gray\", interpolation=\"nearest\")\n",
    "\n",
    "if flags.condition:\n",
    "    img = make_grid(\n",
    "        target, \n",
    "        nrow=math.ceil(flags.batch_size ** 0.5)\n",
    "    ).permute(1, 2, 0).numpy()\n",
    "\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.imshow(img, cmap=\"gray\", interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
